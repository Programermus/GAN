{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "excercise5.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ir2d0lx_qJAk",
        "colab_type": "code",
        "outputId": "ef0d2bdb-9d59-4c84-f1d3-75a05aa47703",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%tensorflow_version 1.x"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xBnxyMK1zjtn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn.functional as nn\n",
        "import torch.autograd as autograd\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.gridspec as gridspec\n",
        "import os\n",
        "from torch.autograd import Variable"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "19CNtRl1zXBa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.examples.tutorials.mnist import input_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ONBGvL3bugwl",
        "colab_type": "code",
        "outputId": "9be8dd65-aa26-4283-deda-07f7d751490e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "mnist = input_data.read_data_sets('../../MNIST_data', one_hot=True)\n",
        "mb_size = 64\n",
        "Z_dim = 100\n",
        "X_dim = mnist.train.images.shape[1]\n",
        "y_dim = mnist.train.labels.shape[1]\n",
        "h_dim = 128\n",
        "c = 0\n",
        "lr = 1e-3\n",
        "\n",
        "\n",
        "def xavier_init(size):\n",
        "    in_dim = size[0]\n",
        "    xavier_stddev = 1. / np.sqrt(in_dim / 2.)\n",
        "    return Variable(torch.randn(*size) * xavier_stddev, requires_grad=True)\n",
        "\n",
        "\n",
        "\"\"\" ==================== GENERATOR ======================== \"\"\"\n",
        "\n",
        "Wzh = xavier_init(size=[Z_dim, h_dim])\n",
        "bzh = Variable(torch.zeros(h_dim), requires_grad=True)\n",
        "\n",
        "Whx = xavier_init(size=[h_dim, X_dim])\n",
        "bhx = Variable(torch.zeros(X_dim), requires_grad=True)\n",
        "\n",
        "\n",
        "def G(z):\n",
        "    h = nn.relu(z @ Wzh + bzh.repeat(z.size(0), 1))\n",
        "    X = nn.sigmoid(h @ Whx + bhx.repeat(h.size(0), 1))\n",
        "    return X\n",
        "\n",
        "\n",
        "\"\"\" ==================== DISCRIMINATOR ======================== \"\"\"\n",
        "\n",
        "Wxh = xavier_init(size=[X_dim, h_dim])\n",
        "bxh = Variable(torch.zeros(h_dim), requires_grad=True)\n",
        "\n",
        "Why = xavier_init(size=[h_dim, 1])\n",
        "bhy = Variable(torch.zeros(1), requires_grad=True)\n",
        "\n",
        "\n",
        "def D(X):\n",
        "    h = nn.relu(X @ Wxh + bxh.repeat(X.size(0), 1))\n",
        "    y = nn.sigmoid(h @ Why + bhy.repeat(h.size(0), 1))\n",
        "    return y\n",
        "\n",
        "\n",
        "G_params = [Wzh, bzh, Whx, bhx]\n",
        "D_params = [Wxh, bxh, Why, bhy]\n",
        "params = G_params + D_params\n",
        "\n",
        "\n",
        "\"\"\" ===================== TRAINING ======================== \"\"\"\n",
        "\n",
        "\n",
        "def reset_grad():\n",
        "    for p in params:\n",
        "        if p.grad is not None:\n",
        "            data = p.grad.data\n",
        "            p.grad = Variable(data.new().resize_as_(data).zero_())\n",
        "\n",
        "\n",
        "G_solver = optim.Adam(G_params, lr=1e-3)\n",
        "D_solver = optim.Adam(D_params, lr=1e-3)\n",
        "\n",
        "ones_label = Variable(torch.ones(mb_size, 1))\n",
        "zeros_label = Variable(torch.zeros(mb_size, 1))\n",
        "\n",
        "\n",
        "for it in range(100000):\n",
        "    # Sample data\n",
        "    z = Variable(torch.randn(mb_size, Z_dim))\n",
        "    X, _ = mnist.train.next_batch(mb_size)\n",
        "    X = Variable(torch.from_numpy(X))\n",
        "\n",
        "    # Dicriminator forward-loss-backward-update\n",
        "    G_sample = G(z)\n",
        "    D_real = D(X)\n",
        "    D_fake = D(G_sample)\n",
        "\n",
        "    D_loss_real = nn.binary_cross_entropy(D_real, ones_label)\n",
        "    D_loss_fake = nn.binary_cross_entropy(D_fake, zeros_label)\n",
        "    D_loss = D_loss_real + D_loss_fake\n",
        "\n",
        "    D_loss.backward()\n",
        "    D_solver.step()\n",
        "\n",
        "    # Housekeeping - reset gradient\n",
        "    reset_grad()\n",
        "\n",
        "    # Generator forward-loss-backward-update\n",
        "    z = Variable(torch.randn(mb_size, Z_dim))\n",
        "    G_sample = G(z)\n",
        "    D_fake = D(G_sample)\n",
        "\n",
        "    G_loss = nn.binary_cross_entropy(D_fake, ones_label)\n",
        "\n",
        "    G_loss.backward()\n",
        "    G_solver.step()\n",
        "\n",
        "    # Housekeeping - reset gradient\n",
        "    reset_grad()\n",
        "\n",
        "    # Print and plot every now and then\n",
        "    if it % 1000 == 0:\n",
        "        print('Iter-{}; D_loss: {}; G_loss: {}'.format(it, D_loss.data.numpy(), G_loss.data.numpy()))\n",
        "\n",
        "        samples = G(z).data.numpy()[:16]\n",
        "\n",
        "        fig = plt.figure(figsize=(4, 4))\n",
        "        gs = gridspec.GridSpec(4, 4)\n",
        "        gs.update(wspace=0.05, hspace=0.05)\n",
        "\n",
        "        for i, sample in enumerate(samples):\n",
        "            ax = plt.subplot(gs[i])\n",
        "            plt.axis('off')\n",
        "            ax.set_xticklabels([])\n",
        "            ax.set_yticklabels([])\n",
        "            ax.set_aspect('equal')\n",
        "            plt.imshow(sample.reshape(28, 28), cmap='Greys_r')\n",
        "\n",
        "        if not os.path.exists('out/'):\n",
        "            os.makedirs('out/')\n",
        "\n",
        "        plt.savefig('out/{}.png'.format(str(c).zfill(3)), bbox_inches='tight')\n",
        "        c += 1\n",
        "        plt.close(fig)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-4-af818515825f>:1: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please write your own downloading logic.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/contrib/learn/python/learn/datasets/base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use urllib or similar directly.\n",
            "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting ../../MNIST_data/train-images-idx3-ubyte.gz\n",
            "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting ../../MNIST_data/train-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.one_hot on tensors.\n",
            "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
            "Extracting ../../MNIST_data/t10k-images-idx3-ubyte.gz\n",
            "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
            "Extracting ../../MNIST_data/t10k-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1569: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iter-0; D_loss: 2.0501208305358887; G_loss: 1.2410434484481812\n",
            "Iter-1000; D_loss: 0.0075107719749212265; G_loss: 9.033361434936523\n",
            "Iter-2000; D_loss: 0.0017367260297760367; G_loss: 10.43954849243164\n",
            "Iter-3000; D_loss: 0.011145001277327538; G_loss: 7.161276817321777\n",
            "Iter-4000; D_loss: 0.050477027893066406; G_loss: 6.111819267272949\n",
            "Iter-5000; D_loss: 0.08895792812108994; G_loss: 6.577515125274658\n",
            "Iter-6000; D_loss: 0.43587106466293335; G_loss: 4.734664440155029\n",
            "Iter-7000; D_loss: 0.7392798662185669; G_loss: 3.2697110176086426\n",
            "Iter-8000; D_loss: 0.8752452731132507; G_loss: 3.052751302719116\n",
            "Iter-9000; D_loss: 0.6643491387367249; G_loss: 3.8166308403015137\n",
            "Iter-10000; D_loss: 0.49512284994125366; G_loss: 2.894050359725952\n",
            "Iter-11000; D_loss: 0.5811561346054077; G_loss: 2.960592031478882\n",
            "Iter-12000; D_loss: 0.6123249530792236; G_loss: 2.5318551063537598\n",
            "Iter-13000; D_loss: 0.7125598192214966; G_loss: 2.4409618377685547\n",
            "Iter-14000; D_loss: 0.7192589044570923; G_loss: 2.1619439125061035\n",
            "Iter-15000; D_loss: 0.6960921287536621; G_loss: 2.3619179725646973\n",
            "Iter-16000; D_loss: 0.48732423782348633; G_loss: 2.306633949279785\n",
            "Iter-17000; D_loss: 0.6450051069259644; G_loss: 2.253598213195801\n",
            "Iter-18000; D_loss: 0.8779215812683105; G_loss: 2.2999048233032227\n",
            "Iter-19000; D_loss: 0.6311712861061096; G_loss: 2.026674747467041\n",
            "Iter-20000; D_loss: 0.6705301403999329; G_loss: 1.9886950254440308\n",
            "Iter-21000; D_loss: 0.818219006061554; G_loss: 1.9084219932556152\n",
            "Iter-22000; D_loss: 0.7944148778915405; G_loss: 2.363381862640381\n",
            "Iter-23000; D_loss: 0.6678226590156555; G_loss: 2.1765332221984863\n",
            "Iter-24000; D_loss: 0.9281266927719116; G_loss: 2.230074644088745\n",
            "Iter-25000; D_loss: 0.813953161239624; G_loss: 1.671708106994629\n",
            "Iter-26000; D_loss: 0.8315622806549072; G_loss: 1.949420690536499\n",
            "Iter-27000; D_loss: 0.8090331554412842; G_loss: 2.3185596466064453\n",
            "Iter-28000; D_loss: 0.6099084615707397; G_loss: 2.1269588470458984\n",
            "Iter-29000; D_loss: 0.6219158172607422; G_loss: 2.103375196456909\n",
            "Iter-30000; D_loss: 0.5450652837753296; G_loss: 2.189375400543213\n",
            "Iter-31000; D_loss: 0.6624590158462524; G_loss: 2.2929468154907227\n",
            "Iter-32000; D_loss: 0.8126022815704346; G_loss: 2.099268674850464\n",
            "Iter-33000; D_loss: 0.8470610976219177; G_loss: 2.3084428310394287\n",
            "Iter-34000; D_loss: 0.6228245496749878; G_loss: 2.6401314735412598\n",
            "Iter-35000; D_loss: 0.6022377014160156; G_loss: 2.7766714096069336\n",
            "Iter-36000; D_loss: 0.5712582468986511; G_loss: 1.7709970474243164\n",
            "Iter-37000; D_loss: 0.7035923004150391; G_loss: 2.0878865718841553\n",
            "Iter-38000; D_loss: 0.6301368474960327; G_loss: 2.7725048065185547\n",
            "Iter-39000; D_loss: 0.5928443074226379; G_loss: 2.0658397674560547\n",
            "Iter-40000; D_loss: 0.6481224894523621; G_loss: 2.4292562007904053\n",
            "Iter-41000; D_loss: 0.5984026193618774; G_loss: 2.175286293029785\n",
            "Iter-42000; D_loss: 0.6374428868293762; G_loss: 2.784175395965576\n",
            "Iter-43000; D_loss: 0.5250694751739502; G_loss: 2.042508840560913\n",
            "Iter-44000; D_loss: 0.6159362196922302; G_loss: 2.3137261867523193\n",
            "Iter-45000; D_loss: 0.6626565456390381; G_loss: 2.800387382507324\n",
            "Iter-46000; D_loss: 0.5794534683227539; G_loss: 2.437246799468994\n",
            "Iter-47000; D_loss: 0.7445074319839478; G_loss: 2.9266409873962402\n",
            "Iter-48000; D_loss: 0.5005403757095337; G_loss: 2.9355292320251465\n",
            "Iter-49000; D_loss: 0.5848917961120605; G_loss: 2.275770902633667\n",
            "Iter-50000; D_loss: 0.6819321513175964; G_loss: 2.9641730785369873\n",
            "Iter-51000; D_loss: 0.5589093565940857; G_loss: 2.4995992183685303\n",
            "Iter-52000; D_loss: 0.5631070733070374; G_loss: 2.4877679347991943\n",
            "Iter-53000; D_loss: 0.6606683731079102; G_loss: 2.2900452613830566\n",
            "Iter-54000; D_loss: 0.5993863344192505; G_loss: 2.5878031253814697\n",
            "Iter-55000; D_loss: 0.5404804348945618; G_loss: 2.53022837638855\n",
            "Iter-56000; D_loss: 0.6539978981018066; G_loss: 2.172220230102539\n",
            "Iter-57000; D_loss: 0.6392987370491028; G_loss: 3.030473470687866\n",
            "Iter-58000; D_loss: 0.6957254409790039; G_loss: 2.8172340393066406\n",
            "Iter-59000; D_loss: 0.6395368576049805; G_loss: 2.3776285648345947\n",
            "Iter-60000; D_loss: 0.5404818654060364; G_loss: 2.3767290115356445\n",
            "Iter-61000; D_loss: 0.6545615196228027; G_loss: 2.5802676677703857\n",
            "Iter-62000; D_loss: 0.5663090944290161; G_loss: 2.7812743186950684\n",
            "Iter-63000; D_loss: 0.5777808427810669; G_loss: 2.8563499450683594\n",
            "Iter-64000; D_loss: 0.6148912906646729; G_loss: 2.876553535461426\n",
            "Iter-65000; D_loss: 0.5848079323768616; G_loss: 2.178169012069702\n",
            "Iter-66000; D_loss: 0.6579960584640503; G_loss: 1.9787746667861938\n",
            "Iter-67000; D_loss: 0.5325807929039001; G_loss: 2.5145938396453857\n",
            "Iter-68000; D_loss: 0.4805271625518799; G_loss: 2.34498929977417\n",
            "Iter-69000; D_loss: 0.5924960970878601; G_loss: 2.6196422576904297\n",
            "Iter-70000; D_loss: 0.6438078880310059; G_loss: 2.549121141433716\n",
            "Iter-71000; D_loss: 0.5520840287208557; G_loss: 2.6730105876922607\n",
            "Iter-72000; D_loss: 0.5188433527946472; G_loss: 1.9846129417419434\n",
            "Iter-73000; D_loss: 0.46635812520980835; G_loss: 2.4224109649658203\n",
            "Iter-74000; D_loss: 0.701076865196228; G_loss: 2.490022659301758\n",
            "Iter-75000; D_loss: 0.3996148705482483; G_loss: 2.5091567039489746\n",
            "Iter-76000; D_loss: 0.5435552597045898; G_loss: 2.487978935241699\n",
            "Iter-77000; D_loss: 0.5710435509681702; G_loss: 2.3032565116882324\n",
            "Iter-78000; D_loss: 0.7060383558273315; G_loss: 2.2401211261749268\n",
            "Iter-79000; D_loss: 0.7091549038887024; G_loss: 2.5199317932128906\n",
            "Iter-80000; D_loss: 0.47612494230270386; G_loss: 2.175746440887451\n",
            "Iter-81000; D_loss: 0.6912744641304016; G_loss: 2.2888283729553223\n",
            "Iter-82000; D_loss: 0.5348423719406128; G_loss: 2.3770835399627686\n",
            "Iter-83000; D_loss: 0.6491460800170898; G_loss: 2.367633819580078\n",
            "Iter-84000; D_loss: 0.5498390197753906; G_loss: 2.626830577850342\n",
            "Iter-85000; D_loss: 0.5521627068519592; G_loss: 2.5459041595458984\n",
            "Iter-86000; D_loss: 0.617367148399353; G_loss: 2.8194851875305176\n",
            "Iter-87000; D_loss: 0.5008646249771118; G_loss: 2.385678291320801\n",
            "Iter-88000; D_loss: 0.6840000152587891; G_loss: 2.391343116760254\n",
            "Iter-89000; D_loss: 0.6855822801589966; G_loss: 2.3116071224212646\n",
            "Iter-90000; D_loss: 0.6912534236907959; G_loss: 2.5712578296661377\n",
            "Iter-91000; D_loss: 0.5673045516014099; G_loss: 2.2353007793426514\n",
            "Iter-92000; D_loss: 0.5275223255157471; G_loss: 2.372131109237671\n",
            "Iter-93000; D_loss: 0.5746545195579529; G_loss: 2.6466944217681885\n",
            "Iter-94000; D_loss: 0.8567068576812744; G_loss: 2.418985366821289\n",
            "Iter-95000; D_loss: 0.6539585590362549; G_loss: 2.1477456092834473\n",
            "Iter-96000; D_loss: 0.5786523818969727; G_loss: 2.6567022800445557\n",
            "Iter-97000; D_loss: 0.6739652752876282; G_loss: 2.6033411026000977\n",
            "Iter-98000; D_loss: 0.4054020643234253; G_loss: 2.3378076553344727\n",
            "Iter-99000; D_loss: 0.512233555316925; G_loss: 2.780588150024414\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KvxmEgxN7vys",
        "colab_type": "code",
        "outputId": "be978cce-ccf2-4ea3-d4a9-1081af067c89",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 462
        }
      },
      "source": [
        "mnist = input_data.read_data_sets('../../MNIST_data', one_hot=True)\n",
        "mb_size = 64\n",
        "Z_dim = 100\n",
        "X_dim = mnist.train.images.shape[1]\n",
        "y_dim = mnist.train.labels.shape[1]\n",
        "h_dim = 128\n",
        "c = 0\n",
        "lr = 1e-3\n",
        "\n",
        "\n",
        "def xavier_init(size):\n",
        "    in_dim = size[0]\n",
        "    xavier_stddev = 1. / np.sqrt(in_dim / 2.)\n",
        "    return Variable(torch.randn(*size) * xavier_stddev, requires_grad=True)\n",
        "\n",
        "\n",
        "\"\"\" ==================== GENERATOR ======================== \"\"\"\n",
        "\n",
        "Wzh = xavier_init(size=[Z_dim, h_dim])\n",
        "bzh = Variable(torch.zeros(h_dim), requires_grad=True)\n",
        "\n",
        "Whx = xavier_init(size=[h_dim, X_dim])\n",
        "bhx = Variable(torch.zeros(X_dim), requires_grad=True)\n",
        "\n",
        "\n",
        "def G(z):\n",
        "    h = nn.relu(z @ Wzh + bzh.repeat(z.size(0), 1))\n",
        "    X = nn.sigmoid(h @ Whx + bhx.repeat(h.size(0), 1))\n",
        "    return X\n",
        "\n",
        "\n",
        "\"\"\" ==================== DISCRIMINATOR ======================== \"\"\"\n",
        "\n",
        "Wxh = xavier_init(size=[X_dim, h_dim])\n",
        "bxh = Variable(torch.zeros(h_dim), requires_grad=True)\n",
        "\n",
        "Why = xavier_init(size=[h_dim, 1])\n",
        "bhy = Variable(torch.zeros(1), requires_grad=True)\n",
        "\n",
        "\n",
        "def D(X):\n",
        "    h = nn.relu(X @ Wxh + bxh.repeat(X.size(0), 1))\n",
        "    y = nn.sigmoid(h @ Why + bhy.repeat(h.size(0), 1))\n",
        "    return y\n",
        "\n",
        "\n",
        "G_params = [Wzh, bzh, Whx, bhx]\n",
        "D_params = [Wxh, bxh, Why, bhy]\n",
        "params = G_params + D_params\n",
        "\n",
        "\n",
        "\"\"\" ===================== TRAINING ======================== \"\"\"\n",
        "\n",
        "\n",
        "def reset_grad():\n",
        "    for p in params:\n",
        "        if p.grad is not None:\n",
        "            data = p.grad.data\n",
        "            p.grad = Variable(data.new().resize_as_(data).zero_())\n",
        "\n",
        "\n",
        "G_solver = optim.Adam(G_params, lr=1e-3)\n",
        "D_solver = optim.Adam(D_params, lr=1e-3)\n",
        "\n",
        "ones_label = Variable(torch.ones(mb_size, 1))\n",
        "zeros_label = Variable(torch.zeros(mb_size, 1))\n",
        "\n",
        "for it in range(100000):\n",
        "    # Sample data\n",
        "    z = Variable(torch.randn(mb_size, Z_dim))\n",
        "    X, _ = mnist.train.next_batch(mb_size)\n",
        "    X = Variable(torch.from_numpy(X))\n",
        "\n",
        "    # Dicriminator forward-loss-backward-update\n",
        "    G_sample = G(z)\n",
        "    D_real = D(X)\n",
        "    D_fake = D(G_sample)\n",
        "\n",
        "    D_loss_real = nn.binary_cross_entropy_with_logits(D_real, ones_label)\n",
        "    D_loss_fake = nn.binary_cross_entropy_with_logits(D_fake, zeros_label)\n",
        "    D_loss = D_loss_real + D_loss_fake\n",
        "\n",
        "    D_loss.backward()\n",
        "    D_solver.step()\n",
        "\n",
        "    # Housekeeping - reset gradient\n",
        "    reset_grad()\n",
        "\n",
        "    # Generator forward-loss-backward-update\n",
        "    z = Variable(torch.randn(mb_size, Z_dim))\n",
        "    G_sample = G(z)\n",
        "    D_fake = D(G_sample)\n",
        "\n",
        "    G_loss = nn.binary_cross_entropy_with_logits(D_fake, ones_label)\n",
        "\n",
        "    G_loss.backward()\n",
        "    G_solver.step()\n",
        "\n",
        "    # Housekeeping - reset gradient\n",
        "    reset_grad()\n",
        "\n",
        "    # Print and plot every now and then\n",
        "    if it % 1000 == 0:\n",
        "        print('Iter-{}; D_loss: {}; G_loss: {}'.format(it, D_loss.data.numpy(), G_loss.data.numpy()))\n",
        "\n",
        "        samples = G(z).data.numpy()[:16]\n",
        "\n",
        "        fig = plt.figure(figsize=(4, 4))\n",
        "        gs = gridspec.GridSpec(4, 4)\n",
        "        gs.update(wspace=0.05, hspace=0.05)\n",
        "\n",
        "        for i, sample in enumerate(samples):\n",
        "            ax = plt.subplot(gs[i])\n",
        "            plt.axis('off')\n",
        "            ax.set_xticklabels([])\n",
        "            ax.set_yticklabels([])\n",
        "            ax.set_aspect('equal')\n",
        "            plt.imshow(sample.reshape(28, 28), cmap='Greys_r')\n",
        "\n",
        "        if not os.path.exists('out/'):\n",
        "            os.makedirs('out/')\n",
        "\n",
        "        plt.savefig('out/{}.png'.format(str(c).zfill(3)), bbox_inches='tight')\n",
        "        c += 1\n",
        "        plt.close(fig)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Extracting ../../MNIST_data/train-images-idx3-ubyte.gz\n",
            "Extracting ../../MNIST_data/train-labels-idx1-ubyte.gz\n",
            "Extracting ../../MNIST_data/t10k-images-idx3-ubyte.gz\n",
            "Extracting ../../MNIST_data/t10k-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1569: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-81fb8d20f671>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0mD_fake\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG_sample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m     \u001b[0mD_loss_real\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary_cross_entropy_with_logits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mD_real\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mones_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m     \u001b[0mD_loss_fake\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary_cross_entropy_with_logits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mD_fake\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzeros_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0mD_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mD_loss_real\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mD_loss_fake\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbinary_cross_entropy_with_logits\u001b[0;34m(input, target, weight, size_average, reduce, reduction, pos_weight)\u001b[0m\n\u001b[1;32m   2431\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2432\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2433\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Target size ({}) must be the same as input size ({})\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2435\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary_cross_entropy_with_logits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction_enum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Target size (torch.Size([64, 1])) must be the same as input size (torch.Size([64, 128]))"
          ]
        }
      ]
    }
  ]
}